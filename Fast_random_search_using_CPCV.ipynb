{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL PATH HERE!\n",
    "path_to_numerai_training_data = 'numerai_training_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import csv\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import ast\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_GROUPS = 6\n",
    "\n",
    "a = list(range(1, NUMBER_OF_GROUPS+1, 1))\n",
    "cv_custom_comb = []\n",
    "test_groups = list(combinations(a, 2))\n",
    "path_dict = dict()\n",
    "for i in range(1, NUMBER_OF_GROUPS+1, 1):\n",
    "    path_dict[i] = 1\n",
    "for g1, g2 in test_groups:\n",
    "    out = (g1, path_dict[g1]), (g2, path_dict[g2])\n",
    "    path_dict[g1] += 1\n",
    "    path_dict[g2] += 1\n",
    "    cv_custom_comb.append(out)\n",
    "cv_custom_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_custom(path):\n",
    "    with open(path, 'r') as f:\n",
    "        column_names = next(csv.reader(f))\n",
    "        dtypes = {x: np.float32 for x in column_names if x.startswith(('feature', 'target'))}\n",
    "    return pd.read_csv(path, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = read_csv_custom(path_to_numerai_training_data)\n",
    "df1['erano'] = df1['era'].apply(lambda x: int(x[3:]))\n",
    "df1['G'] = df1['erano'].apply(lambda x: ((x-1) // 20) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataframe, unimportant=set()):\n",
    "    feature_columns = dataframe.columns\n",
    "    columns_to_remove = []\n",
    "    for feature in feature_columns:\n",
    "        if feature.startswith('target') or feature in ['G', 'prediction', 'id', 'era', 'data_type', 'prediction_kazutsugi', 'erano', 'target_custom', 'preds_neutralized'] or feature in unimportant:\n",
    "            columns_to_remove.append(feature)\n",
    "    feature_columns = feature_columns.drop(columns_to_remove)\n",
    "    return sorted(feature_columns)\n",
    "\n",
    "def get_X_array(df, feature_columns):\n",
    "    return df.loc[:, feature_columns].values\n",
    "\n",
    "def get_Y_array(df):\n",
    "    return df.loc[:, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOURNAMENT_NAME = \"kazutsugi\"\n",
    "TARGET_NAME = \"target\"\n",
    "PREDICTION_NAME = \"prediction\"\n",
    "\n",
    "\n",
    "# Submissions are scored by spearman correlation\n",
    "def score(df):\n",
    "    # method=\"first\" breaks ties based on order in array\n",
    "    return np.corrcoef(\n",
    "        df[TARGET_NAME],\n",
    "        df[PREDICTION_NAME].rank(pct=True, method=\"first\")\n",
    "    )[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 999\n",
    "\n",
    "def get_cat_model(df, hyperparameters, feature_columns, train_eras):\n",
    "    \n",
    "    train = df[df['G'].isin(train_eras)]\n",
    "    dtrain = lgb.Dataset(get_X_array(train, feature_columns), get_Y_array(train), feature_name=feature_columns, free_raw_data=False)\n",
    "    np.random.seed(SEED)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(hyperparameters, dtrain, num_boost_round=10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', ],\n",
    "    'learning_rate': list(np.logspace(np.log10(0.001), np.log10(0.3), base = 10, num = 1000)),\n",
    "    'num_threads': [4, ],\n",
    "    'metric': ['rmse', ],\n",
    "    'verbose': [-1, ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(df, hyperparameters, iteration, best_score, feature_columns=False, neut_columns=False):\n",
    "    \n",
    "    all_res = []\n",
    "    \n",
    "    print(hyperparameters)\n",
    "    if not feature_columns:\n",
    "        feature_columns = get_features(df)\n",
    "    else:\n",
    "        feature_columns = feature_columns\n",
    "    if not neut_columns:\n",
    "        neut_columns = feature_columns\n",
    "    else:\n",
    "        neut_columns = neut_columns\n",
    "\n",
    "    path_dict = defaultdict(list)\n",
    "    \n",
    "    num_paths = 5\n",
    "    num_groups = 6\n",
    "    checked_path = 0\n",
    "    \n",
    "    shr_v_list = [0] * num_paths\n",
    "    mn_v_list = [0] * num_paths\n",
    "    min_v_list = [0] * num_paths\n",
    "    \n",
    "    threshold_value = best_score.mean() - np.std(best_score) * 3\n",
    "    print('threshold_value ', threshold_value)\n",
    "    \n",
    "    for zsp in cv_custom_comb:\n",
    "        \n",
    "        test_eras = set()\n",
    "        train_eras = set()\n",
    "        for i in zsp:\n",
    "            test_eras.add(i[0])\n",
    "        for i in range(1, num_groups+1, 1):\n",
    "            if i not in test_eras:\n",
    "                train_eras.add(i)\n",
    "        model = get_cat_model(df, hyperparameters, feature_columns, train_eras)\n",
    " \n",
    "        for i in zsp:\n",
    "            idx_test = df['G'] == i[0]\n",
    "            df.loc[idx_test, PREDICTION_NAME] = model.predict(get_X_array(df.loc[idx_test, feature_columns], feature_columns))\n",
    "            path_dict[i[1]].append(df.loc[idx_test, [TARGET_NAME, PREDICTION_NAME, 'era']].copy())\n",
    "            \n",
    "        for path_num in range(checked_path+1, num_paths+1, 1):\n",
    "            if len(path_dict[path_num]) >= num_groups:\n",
    "                \n",
    "                test_df6 = pd.concat(path_dict[path_num])\n",
    "                \n",
    "                validation_correlations = test_df6.groupby(\"era\").apply(score)\n",
    "                mn_v = validation_correlations.mean()\n",
    "                std_v = validation_correlations.std()\n",
    "                min_v = validation_correlations.min()\n",
    "                shr_v = mn_v / std_v\n",
    "\n",
    "                shr_v_list[path_num-1] = shr_v\n",
    "                mn_v_list[path_num-1] = mn_v\n",
    "                min_v_list[path_num-1] = min_v\n",
    "                \n",
    "                del path_dict[path_num]\n",
    "                checked_path = path_num\n",
    "                \n",
    "        if checked_path > 0 and any(z1 < threshold_value for z1 in mn_v_list[:checked_path]):\n",
    "            print(mn_v_list[:checked_path])\n",
    "            return [shr_v_list, mn_v_list, min_v_list, hyperparameters, iteration]\n",
    "    \n",
    "    return [shr_v_list, mn_v_list, min_v_list, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(df, param_grid, out_file, max_evals, useful_features=False, neut_columns=False):\n",
    "    \"\"\"Random search for hyperparameter optimization. \n",
    "       Writes result of search to csv file every search iteration.\"\"\"\n",
    "    \n",
    "    random_results = pd.read_csv(out_file)\n",
    "    if len(random_results) > 0:\n",
    "        random_results['score_list'] = random_results['score_list'].apply(lambda x: np.array(literal_eval(x)))\n",
    "        random_results['score_mean'] = random_results['score_list'].apply(lambda x: np.mean(x))\n",
    "        best_score = random_results.sort_values(by='score_mean',ascending=False)['score_list'].values[0]\n",
    "    else:\n",
    "        best_score = np.array([0] * 5)\n",
    "    \n",
    "    eval_results = False\n",
    "\n",
    "    for i in range(max_evals):\n",
    "        \n",
    "        if eval_results is not False:\n",
    "            if np.mean(eval_results[1]) > np.mean(best_score):\n",
    "                best_score = np.array(eval_results[1])\n",
    "        \n",
    "        random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "        \n",
    "        eval_results = objective(df, random_params, i, best_score, useful_features, neut_columns)\n",
    "        print(eval_results[1], best_score)\n",
    "\n",
    "        of_connection = open(out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow(eval_results)\n",
    "        \n",
    "        of_connection.close()\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 10\n",
    "\n",
    "out_file = 'test_02.csv'\n",
    "\n",
    "if not os.path.isfile(out_file):\n",
    "\n",
    "    of_connection = open(out_file, 'w')\n",
    "    writer = csv.writer(of_connection)\n",
    "\n",
    "    # Write column names\n",
    "    headers = ['sharpe_list', 'score_list', 'minval_list', 'params', 'iteration']\n",
    "    writer.writerow(headers)\n",
    "    of_connection.close()\n",
    "\n",
    "\n",
    "random_results = random_search(df1, param_grid, out_file, MAX_EVALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results = pd.read_csv('test_02.csv')\n",
    "\n",
    "for cc in ['sharpe_list', 'score_list', 'minval_list']:\n",
    "    lbl = cc.split('_')[0]\n",
    "    random_results[cc] = random_results[cc].apply(lambda x: np.array(literal_eval(x)))\n",
    "    random_results['%s_mean' % (lbl, )] = random_results['%s_list' % (lbl, )].apply(lambda x: np.mean(x))\n",
    "    random_results['%s_std' % (lbl, )] = random_results['%s_list' % (lbl, )].apply(lambda x: np.std(x))\n",
    "    random_results['%s_max' % (lbl, )] = random_results['%s_list' % (lbl, )].apply(lambda x: np.max(x))\n",
    "    random_results['%s_min' % (lbl, )] = random_results['%s_list' % (lbl, )].apply(lambda x: np.min(x))\n",
    "    \n",
    "random_results['score_count'] = random_results['score_list'].apply(lambda x: sum(np.array(x) > 0))\n",
    "\n",
    "random_results = random_results.sort_values(by='score_mean',ascending=False)\n",
    "bestparams = ast.literal_eval(random_results['params'].values[0])\n",
    "random_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
